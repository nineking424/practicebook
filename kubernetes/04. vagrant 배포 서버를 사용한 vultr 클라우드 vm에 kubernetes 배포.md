vagrant 배포 서버를 사용한 vultr 클라우드 vm에 kubernetes 배포
====
### 배포환경
- **03.** 에서 구축한 kubespray 배포용 vagrant vm을 활용하여 배포
- target 서버 1대로 master & worker
- root 외부 접근 가능
- 111.111.111.111 ip를 실 서버 주소로 치환 하여 진행

### 1. 배포 서버 접속
root에서 진행
```
C:\vagrant\kubespray>vagrant up
...
C:\vagrant\kubespray>vagrant ssh
$ su -
```

### 2. SSH 등록
```
ssh-copy-id root@111.111.111.111
```

### 3. 배포 설정 복사
```
# cd kubespray
# cp -rfp ./inventory/sample ./inventory/vultrvm
# cp /vagrant/vultrvm/hosts.yaml ./inventory/vultrvm/hosts.yaml
# cp /vagrant/vultrvm/addons.yml ./inventory/vultrvm/group_vars/k8s_cluster/addons.yml
```

#### 3-1. hosts.yaml
```yaml
all:
  hosts:
    nkcserver:
      ansible_host: 111.111.111.111
      ip: 111.111.111.111
      access_ip: 111.111.111.111
  children:
    kube_control_plane:
      hosts:
        nkcserver:
    kube_node:
      hosts:
        nkcserver:
    etcd:
      hosts:
        nkcserver:
    k8s_cluster:
      children:
        kube_control_plane:
        kube_node:
    calico_rr:
      hosts: {}
```

#### 3.2 addons.yml
- **03.** 과 동일

### 4. target 서버 접속 확인
```
# ansible all -i inventory/vultrvm/hosts.yaml -m ping
nkcserver | SUCCESS => {
    "changed": false,
    "ping": "pong"
}
```

### 5. 배포
```
# ansible-playbook -i inventory/vultrvm/hosts.yaml --private-key /root/.ssh/id_rsa --become --become-user=root cluster.yml
...
PLAY RECAP *************************************************************************************************************
localhost                  : ok=4    changed=0    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0
nkcserver                  : ok=674  changed=133  unreachable=0    failed=0    skipped=1200 rescued=0    ignored=2
```

### 6. 배포 확인(master 서버에서 실행)
root 유저로 확인하였음(~/.kube/config 생성 skip)
```
root@nkcserver:~# kubectl cluster-info
Kubernetes control plane is running at https://127.0.0.1:6443
```

