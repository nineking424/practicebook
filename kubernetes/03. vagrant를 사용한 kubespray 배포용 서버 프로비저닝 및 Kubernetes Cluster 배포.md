vagrant를 사용한 kubespray 배포용 서버 프로비저닝 및 Kubernetes Cluster 배포
====

**※ 클러스터 서버 구성**
1. 192.168.0.211 - master1
2. 192.168.0.221 - worker1
3. 192.168.0.222 - worker2

### 1. 배포용 서버 Vagrantfile 작성
```ruby
# -*- mode: ruby -*-
# vi: set ft=ruby :

Vagrant.configure("2") do |config|

  config.vm.define "nkc-kubespray" do |cfg|
    cfg.vm.box = "ubuntu/focal64"
    cfg.vm.hostname = "kubespray"
    cfg.vm.network "public_network"
    # manual ip
    cfg.vm.provider "virtualbox" do |vb|
      vb.gui = false
      vb.cpus = 2
      vb.memory = 2048
    end
    cfg.vm.provision "shell", inline: <<-SHELL
      # repository
      sed -i 's/kr.archive.ubuntu.com/mirror.kakao.com/g' /etc/apt/sources.list
      sed -i 's/archive.ubuntu.com/mirror.kakao.com/g' /etc/apt/sources.list
      apt-get update
      apt-get upgrade -y
      apt-get install net-tools
      # root login
      echo -e "yourpassword\nyourpassword" | passwd
      sed  -i 's/#PermitRootLogin yes/PermitRootLogin yes/g' /etc/ssh/sshd_config;
      sed  -i 's/#PermitRootLogin prohibit-password/PermitRootLogin yes/g' /etc/ssh/sshd_config;
      sed  -i 's/PasswordAuthentication no/PasswordAuthentication yes/g' /etc/ssh/sshd_config;
      systemctl restart sshd
      # python
      ln -s /usr/bin/python3.8 /usr/bin/python
      # pip
      apt install -y python3-pip
      # ssh key
      apt install -y sshpass
      ssh-keygen -y -t rsa -N '' -f /root/.ssh/id_rsa <<< n
      key_val=$(cat /root/.ssh/id_rsa.pub)
      rm /root/.ssh/known_hosts
      sshpass -p tjrxo424 ssh -T -o StrictHostKeyChecking=no 192.168.0.211 "echo '$key_val' >> /root/.ssh/authorized_keys"
      sshpass -p tjrxo424 ssh -T -o StrictHostKeyChecking=no 192.168.0.221 "echo '$key_val' >> /root/.ssh/authorized_keys"
      sshpass -p tjrxo424 ssh -T -o StrictHostKeyChecking=no 192.168.0.222 "echo '$key_val' >> /root/.ssh/authorized_keys"
      # kubaspray
      cd /root
      git clone https://github.com/kubernetes-sigs/kubespray.git
      cd kubespray
      yes | pip3 install -r requirements.txt
      sed  -i 's/override_system_hostname: true/override_system_hostname: false/g' ./roles/bootstrap-os/defaults/main.yml
      cp -rfp ./inventory/sample ./inventory/mycluster
      cp /vagrant/hosts.yaml ./inventory/mycluster/hosts.yaml
      cp /vagrant/addons.yml ./inventory/mycluster/group_vars/k8s_cluster/addons.yml
    SHELL
  end
end
```
### 2. kubespray 설정 - (1) hosts.yaml 파일 작성
```yaml
all:
  hosts:
    node1:
      ansible_host: 192.168.0.211
      ip: 192.168.0.211
      access_ip: 192.168.0.211
    node2:
      ansible_host: 192.168.0.221
      ip: 192.168.0.221
      access_ip: 192.168.0.221
    node3:
      ansible_host: 192.168.0.222
      ip: 192.168.0.222
      access_ip: 192.168.0.222
  children:
    kube_control_plane:
      hosts:
        node1:
    kube_node:
      hosts:
        node2:
        node3:
    etcd:
      hosts:
        node1:
    k8s_cluster:
      children:
        kube_control_plane:
        kube_node:
    calico_rr:
      hosts: {}
```
### 3. kubespray 설정 - (2) addons.yml 파일 작성
dashboard_enabled, metrics_server_enabled, ingress_nginx_enabled를 true로 설정하였음
```yaml
---
# Kubernetes dashboard
# RBAC required. see docs/getting-started.md for access details.
dashboard_enabled: true

# Helm deployment
helm_enabled: false

# Registry deployment
registry_enabled: false
# registry_namespace: kube-system
# registry_storage_class: ""
# registry_disk_size: "10Gi"

# Metrics Server deployment
metrics_server_enabled: true
# metrics_server_kubelet_insecure_tls: true
# metrics_server_metric_resolution: 60s
# metrics_server_kubelet_preferred_address_types: "InternalIP"

# Rancher Local Path Provisioner
local_path_provisioner_enabled: false
# local_path_provisioner_namespace: "local-path-storage"
# local_path_provisioner_storage_class: "local-path"
# local_path_provisioner_reclaim_policy: Delete
# local_path_provisioner_claim_root: /opt/local-path-provisioner/
# local_path_provisioner_debug: false
# local_path_provisioner_image_repo: "rancher/local-path-provisioner"
# local_path_provisioner_image_tag: "v0.0.19"
# local_path_provisioner_helper_image_repo: "busybox"
# local_path_provisioner_helper_image_tag: "latest"

# Local volume provisioner deployment
local_volume_provisioner_enabled: false
# local_volume_provisioner_namespace: kube-system
# local_volume_provisioner_nodelabels:
#   - kubernetes.io/hostname
#   - topology.kubernetes.io/region
#   - topology.kubernetes.io/zone
# local_volume_provisioner_storage_classes:
#   local-storage:
#     host_dir: /mnt/disks
#     mount_dir: /mnt/disks
#     volume_mode: Filesystem
#     fs_type: ext4
#   fast-disks:
#     host_dir: /mnt/fast-disks
#     mount_dir: /mnt/fast-disks
#     block_cleaner_command:
#       - "/scripts/shred.sh"
#       - "2"
#     volume_mode: Filesystem
#     fs_type: ext4

# CephFS provisioner deployment
cephfs_provisioner_enabled: false
# cephfs_provisioner_namespace: "cephfs-provisioner"
# cephfs_provisioner_cluster: ceph
# cephfs_provisioner_monitors: "172.24.0.1:6789,172.24.0.2:6789,172.24.0.3:6789"
# cephfs_provisioner_admin_id: admin
# cephfs_provisioner_secret: secret
# cephfs_provisioner_storage_class: cephfs
# cephfs_provisioner_reclaim_policy: Delete
# cephfs_provisioner_claim_root: /volumes
# cephfs_provisioner_deterministic_names: true

# RBD provisioner deployment
rbd_provisioner_enabled: false
# rbd_provisioner_namespace: rbd-provisioner
# rbd_provisioner_replicas: 2
# rbd_provisioner_monitors: "172.24.0.1:6789,172.24.0.2:6789,172.24.0.3:6789"
# rbd_provisioner_pool: kube
# rbd_provisioner_admin_id: admin
# rbd_provisioner_secret_name: ceph-secret-admin
# rbd_provisioner_secret: ceph-key-admin
# rbd_provisioner_user_id: kube
# rbd_provisioner_user_secret_name: ceph-secret-user
# rbd_provisioner_user_secret: ceph-key-user
# rbd_provisioner_user_secret_namespace: rbd-provisioner
# rbd_provisioner_fs_type: ext4
# rbd_provisioner_image_format: "2"
# rbd_provisioner_image_features: layering
# rbd_provisioner_storage_class: rbd
# rbd_provisioner_reclaim_policy: Delete

# Nginx ingress controller deployment
ingress_nginx_enabled: true
# ingress_nginx_host_network: false
ingress_publish_status_address: ""
# ingress_nginx_nodeselector:
#   kubernetes.io/os: "linux"
# ingress_nginx_tolerations:
#   - key: "node-role.kubernetes.io/master"
#     operator: "Equal"
#     value: ""
#     effect: "NoSchedule"
#   - key: "node-role.kubernetes.io/control-plane"
#     operator: "Equal"
#     value: ""
#     effect: "NoSchedule"
# ingress_nginx_namespace: "ingress-nginx"
# ingress_nginx_insecure_port: 80
# ingress_nginx_secure_port: 443
# ingress_nginx_configmap:
#   map-hash-bucket-size: "128"
#   ssl-protocols: "TLSv1.2 TLSv1.3"
# ingress_nginx_configmap_tcp_services:
#   9000: "default/example-go:8080"
# ingress_nginx_configmap_udp_services:
#   53: "kube-system/coredns:53"
# ingress_nginx_extra_args:
#   - --default-ssl-certificate=default/foo-tls
# ingress_nginx_class: nginx

# ambassador ingress controller deployment
ingress_ambassador_enabled: false
# ingress_ambassador_namespace: "ambassador"
# ingress_ambassador_version: "*"
# ingress_ambassador_multi_namespaces: false

# ALB ingress controller deployment
ingress_alb_enabled: false
# alb_ingress_aws_region: "us-east-1"
# alb_ingress_restrict_scheme: "false"
# Enables logging on all outbound requests sent to the AWS API.
# If logging is desired, set to true.
# alb_ingress_aws_debug: "false"

# Cert manager deployment
cert_manager_enabled: false
# cert_manager_namespace: "cert-manager"

# MetalLB deployment
metallb_enabled: false
metallb_speaker_enabled: true
# metallb_ip_range:
#   - "10.5.0.50-10.5.0.99"
# metallb_speaker_nodeselector:
#   kubernetes.io/os: "linux"
# metallb_controller_nodeselector:
#   kubernetes.io/os: "linux"
# metallb_speaker_tolerations:
#   - key: "node-role.kubernetes.io/master"
#     operator: "Equal"
#     value: ""
#     effect: "NoSchedule"
#   - key: "node-role.kubernetes.io/control-plane"
#     operator: "Equal"
#     value: ""
#     effect: "NoSchedule"
# metallb_controller_tolerations:
#   - key: "node-role.kubernetes.io/master"
#     operator: "Equal"
#     value: ""
#     effect: "NoSchedule"
#   - key: "node-role.kubernetes.io/control-plane"
#     operator: "Equal"
#     value: ""
#     effect: "NoSchedule"
# metallb_version: v0.9.6
# metallb_protocol: "layer2"
# metallb_port: "7472"
# metallb_limits_cpu: "100m"
# metallb_limits_mem: "100Mi"
# metallb_additional_address_pools:
#   kube_service_pool:
#     ip_range:
#       - "10.5.1.50-10.5.1.99"
#     protocol: "layer2"
#     auto_assign: false
# metallb_protocol: "bgp"
# metallb_peers:
#   - peer_address: 192.0.2.1
#     peer_asn: 64512
#     my_asn: 4200000000
#   - peer_address: 192.0.2.2
#     peer_asn: 64513
#     my_asn: 4200000000

# The plugin manager for kubectl
krew_enabled: false
krew_root_dir: "/usr/local/krew"
```

### 4. vagrant up
```
C:\vagrant\kubespray>vagrant up
```

### 5. 설치 확인
`vagrant ssh` 사용하여 접속 후 root로 진행
```
# cd kubespray
# ansible all -i inventory/mycluster/hosts.yaml -m ping
master1 | SUCCESS => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python"
    },
    "changed": false,
    "ping": "pong"
}
worker2 | SUCCESS => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3"
    },
    "changed": false,
    "ping": "pong"
}
worker1 | SUCCESS => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3"
    },
    "changed": false,
    "ping": "pong"
}
```

### 6. Kubernetes Cluster 배포
```
# ansible-playbook -i inventory/mycluster/hosts.yaml --private-key /root/.ssh/id_rsa --become --become-user=root cluster.yml
```


**※ 참고문서**
- [Vagrant] SSH 공개키 자동 등록 환경 설정, Ansible 사용 : https://myjamong.tistory.com/242
- [Linux] sshpass - 리눅스 (우분투) ssh 접속 시 패스워드를 미리 입력 : https://harryp.tistory.com/624
